# Multimodal Deepfake Detection with XAI

![Status](https://img.shields.io/badge/Status-Work_in_Progress-yellow)
![Language](https://img.shields.io/badge/Language-Python%20%7C%20Jupyter-blue)
![University](https://img.shields.io/badge/UFE-Professional_Project-red)

>  Note: This project is currently under active development. Features and documentation are subject to change.

This repository hosts the source code for a **Multimodal Deepfake Detection System**, developed as part of the **Professional Project Course** at the **Université française d'Égypte (UFE)**.
The system aims to detect manipulated media by analyzing both **visual** and **auditory** cues. It integrates **Explainable AI (XAI)** techniques to provide transparent reasoning behind its detection results, helping users understand *why* a specific media file was flagged as fake.

### Key Features (In Progress)
* **Multimodal Analysis:** Combines video frame analysis with audio frequency processing for robust detection.
* **Explainable AI (XAI):** Visualizes decision-making layers (e.g., heatmaps) to highlight manipulated regions.
* **Deep Learning Pipelines:** Custom CNN architectures and pre-trained models.
* **Interactive UI:** A user-friendly interface (via Gradio) to upload and test media files.

## Built With

* **Python** (Primary Language)
* **Jupyter Notebooks** (Development Environment)
* **Deep Learning Frameworks:** PyTorch
* **Interface:** Gradio

### Contributing
As this is an academic project, direct contributions are restricted to the project team. However, feedback and suggestions are welcome!
